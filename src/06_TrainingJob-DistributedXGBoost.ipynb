{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32a5b22d-06b4-4dd5-9205-250969151c6f",
   "metadata": {},
   "source": [
    "# Distributed XGBoost Training Job\n",
    "Use XGBoost distributed training on multiple instances managed by sagemaker to train and evaluate against 35M+ rows of 2023 and 10M rows for first quarter 2024 respectively\n",
    "\n",
    "- Using category enabled fields in XGBoost supported in version > 1.5. No need to convert to onehot encoded factors for categorical variables\n",
    "\n",
    "### Data at scale bottlenecks:\n",
    "- Tree Memory: RAM requirements grows exponentially with depth of the tree, so you need to limit the depth of the tree, keep it around 10 or restrict the number of leaves to 1024 or less.\n",
    "- Restrict the distinct categories in the categorical variables as XGBoost assess split on every subset of the categories, if thousands of categories, XGBoost will choke\n",
    "\n",
    "### XGBoost Estimator; Points to Note:\n",
    "- When instance_count is set to grt 1, Distributed training will automatically turns on\n",
    "- Uses RABIT protocol for gradient sharing across shards on different instances to calculate global gradients\n",
    "- Automatically calculates the global accuracy of models only for implicit evaluation inside xgboost, it won't do calculation when you write explicit code for evaluation.\n",
    "- Similarly, when processing data in training script, it will process data locally only and it won't sync globally, i.e. one hot encoding, different number of columns if different number of distinct categories in the shard.\n",
    "- So keep data processing, explicit evaluation out of the trainig script, keep it minimal only for training\n",
    "- Keep a consistent schema across all the shards, utilize a data processing step for that\n",
    "- Though you can do statless transformation operation in the training script, but it should keep the schema consistent across all steps\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "id": "78386b16-1059-4bba-b431-d707f238088a",
   "metadata": {},
   "source": [
    "import sagemaker\n",
    "print(sagemaker.__version__)\n",
    "# If < 2.27.0, upgrade:\n",
    "#!pip install --upgrade sagemaker"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ad6e6846-1e2d-449a-a3c1-228791cfa574",
   "metadata": {},
   "source": [
    "import sagemaker \n",
    "import boto3 \n",
    "import pandas as pd \n",
    "from datetime import datetime"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "42d8595d-5f94-4267-8d88-cd13167c8a3a",
   "metadata": {},
   "source": [
    "REGION = sagemaker.session.Session().boto_region_name\n",
    "print(\"REGION: \", REGION) \n",
    "\n",
    "boto3_session = boto3.Session(region_name=REGION)\n",
    "\n",
    "sagemaker_boto3_client = boto3_session.client(\"sagemaker\")\n",
    "s3_boto3_client = boto3_session.client(\"s3\")\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=boto3_session, sagemaker_client=sagemaker_boto3_client)\n",
    "\n",
    "BUCKET = sagemaker_session.default_bucket()\n",
    "PREFIX = \"NYC_Taxi_Prediction\"\n",
    "\n",
    "ROLE=sagemaker.get_execution_role()\n",
    "print(\"ROLE: \", ROLE)\n",
    "print(\"BUCKET: \", BUCKET) \n",
    "print(\"PREFIX: \", PREFIX) \n",
    "\n",
    "s3_dir_uri = f\"s3://{BUCKET}/{PREFIX}\"\n",
    "print(s3_dir_uri)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f42765e2-e497-430b-bb19-3f3fd9cab1a4",
   "metadata": {},
   "source": [
    "# Training Job Output Dir\n",
    "s3_estimator_out_dir_uri = f'{s3_dir_uri}/02_training_jobs/'\n",
    "print(s3_estimator_out_dir_uri)\n",
    "\n",
    "# Training Job input data dir\n",
    "s3_dataprocess_out_dir_uri = 's3://sagemaker-us-east-1-205930620783/NYC_Taxi_Prediction/01_dataprocessing_jobs/NYC-Taxi-Prediction-2025-08-20-14-24-53'\n",
    "train_dir = f'{s3_dataprocess_out_dir_uri}/train/'\n",
    "test_dir = f'{s3_dataprocess_out_dir_uri}/test/'\n",
    "print(train_dir)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b0e38e98-2d7d-42fd-91f9-bcfb12b657b1",
   "metadata": {},
   "source": [
    "## Training Step\n",
    "#### First test the 'xgbost_model_script.py' on the local data"
   ]
  },
  {
   "cell_type": "code",
   "id": "57ffd4d1-0c43-43b8-8d53-32fb77c0d4d9",
   "metadata": {},
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "# Download the small data from s3\n",
    "#sagemaker_session.download_data(path='data/', bucket=\"sagemaker-us-east-1-205930620783\", key_prefix=\"NYC_Taxi_Prediction/01_dataprocessing_jobs/NYC-Taxi-Prediction-2025-08-14-12-25-26\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2480271e-596b-4763-bfd3-2fd37b7047f3",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "!python scripts/xgboost_model_script.py \\\n",
    "--train-dir \"./data/train/\" \\\n",
    "--test-dir \"./data/test/\" \\\n",
    "--model-out-dir \"./data/model/\" \\\n",
    "--model-data-out-dir \"./data/model\" \\\n",
    "--target-var \"trip_duration_mins\" \\\n",
    "--features \"features|passenger_count|trip_distance|vendorid|ratecodeid|day_of_week|day_of_month|month_of_year|hour_of_day|week_of_year\" \\\n",
    "--num-boost-round 10 \\\n",
    "--max-depth 5 \\\n",
    "--eta 0.1 \\\n",
    "--objective \"reg:squarederror\" \n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"Execution time: {elapsed:.2f} seconds\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1effa71a-6149-485d-a77d-2f045a5f8984",
   "metadata": {},
   "source": [
    "###  How multi-node training works in SageMaker XGBoost\n",
    "- Training Phase\n",
    "  - Data sharding\n",
    "    - SageMaker automatically splits your S3 dataset into chunks, one per node.\n",
    "    - Example: If you have 4 instances and train.libsvm has 400,000 rows → each worker gets ~100,000 rows.\n",
    "    - Sharding is handled by the SageMaker channel input configuration, so you don’t have to code for it.\n",
    "\n",
    "  - Rabit initialization\n",
    "    SageMaker sets environment variables for distributed training:\n",
    "\n",
    "    DMLC_ROLE=worker\n",
    "    \n",
    "    DMLC_NUM_WORKER=<instance_count>\n",
    " \n",
    "    DMLC_TRACKER_URI=<master_node_ip>\n",
    " \n",
    "    DMLC_TRACKER_PORT=9091\n",
    " \n",
    "    XGBoost detects these and starts Rabit, which sets up synchronous allreduce communication between nodes.\n",
    "\n",
    "\n",
    "  - Local gradient computation\n",
    "    - Each worker trains on its local shard, computing gradient and hessian statistics for its subset of the data.\n",
    "    - Gradient aggregation: Rabit performs an allreduce operation to sum gradients and hessians across all workers.\n",
    "    - Each worker gets the global sum, so they make identical split decisions.\n",
    "    This is why every node builds the same model.\n",
    "\n",
    "\n",
    "- Evaluation Phase\n",
    "    - Local evaluation\n",
    "        - For each dataset you pass in evals (train, test), each worker computes the evaluation metric (RMSE, logloss, etc.) on its shard only.\n",
    "    - Global aggregation\n",
    "        - Rabit allreduces the partial sums (e.g., sum of squared errors) and counts across workers.\n",
    "        - It then computes the global metric that reflects all shards combined.\n",
    "    This is why your train and eval metrics in eval_results are identical on every node — they are global metrics.\n",
    "\n",
    "\n",
    "#### This means:\n",
    "The RMSE you see in multi-node training is exactly the same as if you trained on one machine with the whole dataset. There is no “per-node” metric; everything is aggregated.\n",
    "\n",
    "\n",
    "#### Parameters to enable multi-node training in SageMaker\n",
    "There is no special hyperparameter you pass to XGBoost to turn on multi-node training — it’s activated automatically if:\n",
    "- instance_count > 1 in your Estimator.\n",
    "- You’re using an AWS built-in XGBoost container framework_version >= 0.90-2 (recommended: 1.5-1 or newer).\n",
    "- Your input data is in S3 and provided via the SageMaker fit() call (so it can shard automatically).\n",
    "\n",
    "#### How it works in multi-node\n",
    "XGBoost use all cpus available in the instance by default.\n",
    "- Multi-node parallelism (Rabit) — distributes data shards across multiple instances.\n",
    "- Multi-threaded execution within each node — uses all CPU cores on that node to process its shard faster."
   ]
  },
  {
   "cell_type": "code",
   "id": "4dda46e0-2413-4570-9803-6efd98cac8a4",
   "metadata": {},
   "source": [
    "from sagemaker.xgboost.estimator import XGBoost \n",
    "from sagemaker.inputs import TrainingInput \n",
    "\n",
    "xgb_hyperparams = {\n",
    "    'target-var': 'trip_duration_mins', \n",
    "    #'features': \"features|passenger_count|trip_distance|vendorid|ratecodeid|day_of_week|day_of_month|month_of_year|hour_of_day|week_of_year\", \n",
    "    #'features': \"pick_drop_loc|passenger_count|trip_distance|vendorid|day_of_week|day_of_month|month_of_year|hour_of_day|week_of_year\", \n",
    "    'features': \"trip_distance\", # Is the most important feature here, and almost all performance coming from this factor\n",
    "    'max-depth':15, \n",
    "    'max-leaves':1024, \n",
    "    #'eta':0.3, \n",
    "    'objective':'reg:squarederror',\n",
    "    'num-boost-round': 100\n",
    "}\n",
    "xgb_estimator = XGBoost(\n",
    "    framework_version=\"1.5-1\",\n",
    "    entry_point=\"scripts/v2-xgboost_model_script.py\",\n",
    "    output_path=s3_estimator_out_dir_uri,       # For model file and metrics file. \n",
    "    code_location=s3_estimator_out_dir_uri, # If not provided it will be put in the default bucket\n",
    "    hyperparameters=xgb_hyperparams,\n",
    "    role=ROLE,\n",
    "    instance_count=16,\n",
    "    instance_type=\"ml.r5.2xlarge\",#\"ml.m5.2xlarge\",#\"ml.r5.xlarge\",#\"ml.m5.xlarge\",\n",
    "    max_run=3600#,\n",
    "    #base_job_name=f'NYC_Taxi_Prediction-Training'\n",
    ")\n",
    "\n",
    "xgb_estimator.fit(\n",
    "    inputs={\n",
    "        # Copies s3 data to SM_CHANNEL_TRAIN i.e. /opt/ml/input/data/train. Needs to have enough disk to accomodate large files.\n",
    "        # can read directly from s3 files by providing the s3 uri to script, but s3 reads are slower v/s local reads.\n",
    "        'train': TrainingInput(train_dir, distribution=\"ShardedByS3Key\"), \n",
    "        'test': TrainingInput(test_dir, distribution=\"ShardedByS3Key\")\n",
    "})\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045fa806-7649-422b-a1b5-58d03ec25bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
