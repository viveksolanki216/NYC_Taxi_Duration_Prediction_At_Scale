{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Processing Job\n",
    "Pre-process the 2023 trip data for training (35M+ rows) and first quarter of 2024 (10M+ rows) data for testing using spark cluster and PySpark\n",
    "\n",
    "### What Does this notebook do?\n",
    "- Runs a pyspark script using PySparkProcessor on a spark cluster\n",
    "- Production grade data processing:\n",
    "    - Processes traing data first, stores the onhotencode and other transformation object to process test data\n",
    "- PySpark Script\n",
    "    - Input: Glue Data Catalog Table that represents schema unified NYC data from different monthly parquet files (generated from an ETL Glue job)\n",
    "    - Ouptut: Processed/Transformed data, randomly partitioned to 128 shards.\n",
    "    - Processing/Transformation:\n",
    "        - Calculate Target\n",
    "        - Filter Outliers\n",
    "        - Extract Seasonality features i.e hour_of_the_day, day_of_the_week etc\n",
    "        - Assigne rare categories to 'Other'\n",
    "        - Encodes categorical features to OneHotEncoded variables stored in as vectors under single column \"features\"\n",
    "        - Generates a summary for 0.1% sample of the data\n",
    "        - Store metadata for features i.e. target/features names\n",
    "        - Storing Preprocessr details to use to transform test data\n",
    "                - Storing Pipeline=[\n",
    "\n",
    "#### Pyspark Feature Extraction and Transformations:\n",
    "https://spark.apache.org/docs/latest/ml-features.html\n",
    "\n",
    "**Important Operations used here:**\n",
    "- StringIndexer: Encodes string type column to indices/numbers based on frequency of strings i.e. if \"hot\" is most frequent strings it encodes it 0, then next frequent word to 1 and so on.\n",
    "- OneHotEncoder: First you need to encode the strings/categories to indices using StringIndexer\n",
    "- VectorAssembler: Assembles all ohe coded vectors into a single vector and put it in single column called \"features\" along with other numerical feautres in the dataframe.\n",
    "- GroupBy: To create summaries\n",
    "\n",
    "**Advices:**\n",
    "- Do all pre-processing especially stateful (that needs a scan over whole dataset) transformations operations on data before distributed training, as processing is not synchornized over multi-node training over XGBoost estimator framework.\n",
    "- Use parquet files, smaller in size, faster read and writes v/s csv. pandas.read_parque: just provide the directory and it will read all files in dir, sub dir, sub-sub dirs etc v/s pandas.read_csv doesn't support that."
   ],
   "id": "e87d716e-6a0b-4882-b69c-89cc48a28d62"
  },
  {
   "cell_type": "code",
   "id": "78386b16-1059-4bba-b431-d707f238088a",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import sagemaker\n",
    "print(sagemaker.__version__)\n",
    "# If < 2.27.0, upgrade:\n",
    "#!pip install --upgrade sagemaker"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad6e6846-1e2d-449a-a3c1-228791cfa574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker \n",
    "import boto3 \n",
    "import pandas as pd \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "id": "42d8595d-5f94-4267-8d88-cd13167c8a3a",
   "metadata": {},
   "source": [
    "REGION = sagemaker.session.Session().boto_region_name\n",
    "print(\"REGION: \", REGION) \n",
    "\n",
    "boto3_session = boto3.Session(region_name=REGION)\n",
    "\n",
    "sagemaker_boto3_client = boto3_session.client(\"sagemaker\")\n",
    "s3_boto3_client = boto3_session.client(\"s3\")\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=boto3_session, sagemaker_client=sagemaker_boto3_client)\n",
    "\n",
    "BUCKET = sagemaker_session.default_bucket()\n",
    "PREFIX = \"NYC_Taxi_Prediction\"\n",
    "\n",
    "ROLE=sagemaker.get_execution_role()\n",
    "print(\"ROLE: \", ROLE)\n",
    "print(\"BUCKET: \", BUCKET) \n",
    "print(\"PREFIX: \", PREFIX) \n",
    "\n",
    "s3_dir_uri = f\"s3://{BUCKET}/{PREFIX}\"\n",
    "print(s3_dir_uri)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "164c1ba5-fb46-48ab-b473-57a1be6537bc",
   "metadata": {},
   "source": [
    "## Pipeline Parameters\n",
    "\n",
    "Pipeline parameters are conceptually similar to command-line arguments (argparse) in a Python script. Both allow external users or systems to provide input values at runtime instead of hardcoding them.\n",
    "\n",
    "As well, unlike command line args, these \"Parameters\" are automatically logged and tracked. \n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f42765e2-e497-430b-bb19-3f3fd9cab1a4",
   "metadata": {},
   "source": [
    "# current timestamp\n",
    "current_timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\") \n",
    "print(current_timestamp)\n",
    "\n",
    "# Pipeline Name and it's execution name\n",
    "pipeline_name = \"NYC-Taxi-Prediction\"\n",
    "pipeline_execution_name = f'{pipeline_name}-{current_timestamp}'\n",
    "\n",
    "# Data Processing Parameters\n",
    "s3_dataprocess_out_dir_uri = f'{s3_dir_uri}/01_dataprocessing_jobs/{pipeline_execution_name}'\n",
    "print(s3_dataprocess_out_dir_uri)\n",
    "#s3_dataprocess_out_dir_uri = \"s3://sagemaker-us-east-1-205930620783/NYC_Taxi_Prediction/01_dataprocessing_jobs/NYC-Taxi-Prediction-2025-08-15-03-34-04\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2c5d19d1-4f88-4813-92d6-27a0333a88ba",
   "metadata": {},
   "source": [
    "## Data Preprocessing Step\n",
    "\n",
    "# PySparkProcessor (v/s SKLearnProcessor)\n",
    "It's recommended to use PySparkProcessor for distributed data processing using spark, as SKLearnProcessor doesn't support distributed processing unless the input script handles it. So even if you provide instance_count > 1 for SKLearnProcessor it still will use a single instance for the job.\n",
    "\n",
    "While for distributed training, you can use XGBoost estimator and set parameters like instance_count>1 and/or S3ShardKey. The XGBoost estimator framework/Sagemaker automatically launches multiple instances and use RABIT protocol for distributed tree building and gradient computation among instances. \n",
    "\n",
    "You can use Spark ML Framework too i.e. MLib for distributed computing using \"PySparkProcessor\" too. But only use it for specific use-cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "fa081c1a-c822-43ae-a900-60231ca3c827",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "pyspark_processor = PySparkProcessor(\n",
    "    base_job_name=f'{pipeline_name}-DataProc-train',\n",
    "    framework_version='3.3',\n",
    "    role=ROLE,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=4,\n",
    "    max_runtime_in_seconds=1200\n",
    "    #dependency_location=,\n",
    ")\n",
    "\n",
    "pyspark_processor.run(\n",
    "    inputs=[],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            source=\"/opt/ml/processing/output/\",\n",
    "            destination=s3_dataprocess_out_dir_uri\n",
    "        )\n",
    "    ],\n",
    "    submit_app=\"scripts/pyspark_data_processing.py\",\n",
    "    arguments=[\n",
    "        \"--mode\", \"train\",\n",
    "        \"--database-name\", \"nyc_taxi_data\",\n",
    "        \"--table-name\", \"schema_corrected_data\",\n",
    "        \"--num-shards\", \"128\",\n",
    "        \"--start-date\", \"2023-01-01\",\n",
    "        \"--end-date\", \"2023-12-31\",\n",
    "        \"--out-path\", f\"{s3_dataprocess_out_dir_uri}\"\n",
    "    ]#,spark_event_logs_s3_uri=s3_dataprocess_out_dir_uri\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "87cbb26b-dcb7-44ba-a36e-c2646ac99ed4",
   "metadata": {},
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "pyspark_processor = PySparkProcessor(\n",
    "    base_job_name=f'{pipeline_name}-DataProc-test',\n",
    "    framework_version='3.3',\n",
    "    role=ROLE,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=4,\n",
    "    max_runtime_in_seconds=1200\n",
    ")\n",
    "\n",
    "pyspark_processor.run(\n",
    "    submit_app=\"scripts/pyspark_data_processing.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=f\"{s3_dataprocess_out_dir_uri}/common_categories_pick_drop_loc.json\",\n",
    "            destination=\"/opt/ml/processing/input/\"\n",
    "        )\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--mode\", \"test\",\n",
    "        \"--database-name\", \"nyc_taxi_data\",\n",
    "        \"--table-name\", \"schema_corrected_data\",\n",
    "        \"--num-shards\", \"128\",\n",
    "        \"--start-date\", \"2024-01-01\",\n",
    "        \"--end-date\", \"2024-03-30\",\n",
    "        \"--out-path\", f\"{s3_dataprocess_out_dir_uri}\"\n",
    "    ]\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc5a90b-d1e3-4e65-b44c-e8d450dcdd65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
